# -*- coding: utf-8 -*-
"""Cópia de ArvoresDeDecisao_montains_beach.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17iXjS3PzDhqBoQcC7tk3_G7c3C5wPuaH

**Importação das bibliotecas básicas**
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from yellowbrick.classifier import ConfusionMatrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

from google.colab import drive
drive.mount('/content/drive')

base_montains = pd.read_excel('/content/drive/MyDrive/Dados/montains_beach.xlsx', engine='openpyxl')

base_montains

base_montains.shape

X_base_montains = base_montains.iloc[:, 0:8].values
X_base_montains

y_base_montains = base_montains.iloc[:, 0:8].values
y_base_montains

base_montains.describe()

"""**Visualização dos dados**"""

np.unique(base_montains['Gender'], return_counts=True)

np.unique(base_montains['Education_Level'], return_counts=True)

np.unique(base_montains['Preferred_Activities'], return_counts=True)

np.unique(base_montains['Location'], return_counts=True)

np.unique(base_montains['Favorite_Season'], return_counts=True)

np.unique(base_montains['Pets'], return_counts=True)

np.unique(base_montains['Environmental_Concerns'], return_counts=True)

np.unique(base_montains['Preference'], return_counts=True)

sns.countplot(x = base_montains['Preferred_Activities']);

X_preferencias = base_montains.iloc[:, 2:3]

# consulta de valores nan na base
base_montains[base_montains.isnull().any(axis=1)]

base_montains.shape

X_categoricas = base_montains.iloc[:, 0:5]

# Passo 1: Selecionar as variáveis categóricas, exceto a coluna 'Education_Level'
X_categoricas = base_montains.drop(columns=['Education_Level'])  # Exclui 'Education_Level'

X_categoricas

from sklearn.preprocessing import OneHotEncoder

# Passo 1: Selecionar a coluna 'Education_Level'
X_education = base_montains[['Education_Level']]  # Seleciona 'Education_Level' como DataFrame

# Inicializando o OneHotEncoder
encoder = OneHotEncoder(sparse_output=False, drop='first')  # drop='first' para evitar multicolinearidade

# Passo 2: Ajustar o encoder e transformar a variável 'Education_Level'
X_education_encoded = encoder.fit_transform(X_education)

# Converter o numpy.ndarray para DataFrame com as colunas apropriadas
X_education_encoded_df = pd.DataFrame(X_education_encoded,
                                      columns=encoder.get_feature_names_out(['Education_Level']))

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()

# Copiar X_categoricas para não alterar o original
X_categoricas_encoded = X_categoricas.copy()

# Aplicar o LabelEncoder em todas as colunas categóricas (0 a 4)
for coluna in X_categoricas_encoded.columns:
    X_categoricas_encoded[coluna] = encoder.fit_transform(X_categoricas_encoded[coluna])

X_categoricas_encoded



# Separando as variáveis de entrada (X) e a variável alvo (y)

# Variáveis de entrada: Colunas categóricas codificadas + variáveis numéricas
# X_categoricas_encoded já contém as variáveis categóricas codificadas
X_numericas = base_montains.iloc[:, 5:7]  # Variáveis numéricas "Pets" e "Environmental_Concerns"

X_numericas

# Separando os dados de entrada (X) e saída (y)
# Concatenar as variáveis categóricas codificadas com as variáveis numéricas
X_entrada = pd.concat([X_categoricas_encoded, X_numericas,X_education_encoded_df], axis=1)
y_saida = base_montains.iloc[:, 7].values

X_entrada

y_saida

"""**Divisão entre previsores e classe**"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# Dividir os dados em treino e teste (70% treino, 30% teste)
X_train, X_test, y_train, y_test = train_test_split(X_entrada, y_saida, test_size=0.2, random_state=42, stratify=y_saida)

base_montains.describe()

## Criando o modelo com parâmetros ajustados
arvore_montains = DecisionTreeClassifier(criterion='entropy', max_depth=5, class_weight='balanced')
#ajusta o modelo da Árvore de Decisão aos dados de treinamento fornecidos em X_risco_credito (atributos ou características dos dados) e
#y_risco_credito (rótulos ou classes dos dados)
arvore_montains.fit(X_train, y_train)

#fornece a importância das características (features) na tomada de decisão do modelo
arvore_montains.feature_importances_

#fornece as classes
arvore_montains.classes_

base_montains.columns

from sklearn import tree
#Instead of manually defining the features, get them from the dataframe used to train the model:
# Assuming X_train is a pandas DataFrame:
previsores = X_train.columns

# Convert class names to strings
class_names_str = [str(c) for c in arvore_montains.classes_]

figura, eixos = plt.subplots(nrows=1, ncols=1, figsize=(8,20))
tree.plot_tree(arvore_montains, feature_names=previsores, class_names = class_names_str , filled=True);

#predição com a base de treinamento
previsoes = arvore_montains.predict(X_test)
previsoes

#classes de treinamento
y_test

previsoes

#acurácia do modelo analisando com a base de treinamento
accuracy_score(y_test, previsoes)

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, previsoes)
print(cm)

base_montains.shape

base_montains.head(5)

from sklearn.metrics import classification_report
print(classification_report(y_test, previsoes))

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, previsoes)
print("Matriz de Confusão:")
print(cm)

from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# Criar o modelo de Árvore de Decisão
arvore_montains = DecisionTreeClassifier(criterion='entropy')

# Determine the minimum number of samples in any class of y_saida
min_samples_per_class = np.min(np.bincount(y_saida))

# Use cross-validation with a suitable number of folds
# Ensure cv is less than or equal to min_samples_per_class
num_folds = min(10, min_samples_per_class)  # Use min to ensure cv is valid
scores = cross_val_score(arvore_montains, X_entrada, y_saida, cv=num_folds, scoring='accuracy')

# Exibir os resultados
print("Acurácia para cada fold:", scores)
print("Acurácia média:", np.mean(scores))